# CarND-Behavioral-Cloning

The goals / steps of this project are the following:  
•	Use the simulator to collect data of good driving behaviour  
•	Build, a convolution neural network in Keras that predicts steering angles from images  
•	Train and validate the model with a training and validation set  
•	Test that the model successfully drives around track one without leaving the road 
•	Summarize the results with a written report  
Project includes the following files:  
•	model.py containing the script to create and train the model  
•	drive.py for driving the car in autonomous mode  
•	model.h5 containing a trained convolution neural network  
•	writeup_report.md or writeup_report.pdf summarizing the results  
•	video.mp4 which shows the lap around the track in autonomous mode  
Description of the program:  
Before running the code, training data must be recorded. For this, the simulator was started, and a few laps were recorded in counter-clockwise direction. Another round was recorded in clockwise direction, which is basically a completely new track for the program to learn steering angles. Also, recovery data was recorded so the car knows what to do if it is too close to the lane markings. I recorded recovery data from normal yellow lane markings, from the red-white markings, at the bridge and at the off-road sections.
Since recording the training data was difficult due to an extremely low number of frames per second, the resulting quality was not good enough and the vehicle was not able to drive safely around the track. Therefore, I decided to use the training data provided by Udacity to train the model. This dataset consists of 8036 frames. There are images from the centre camera, as well as from a camera on the left and on the right side for each frame, which results in a total number of 24108 Images for training. Table 1 shows an image from each of the cameras from one frame.

To get a better insight into the training data, figure 1 shows the histogram for steering angles. It is striking, that the dataset has around 4400 images with a steering angle close to zero. Only very few images have a maximum steering angle in left or right direction. It is also visible, that the left and right side are very balanced. This is good to know because this way, there is no augmentation needed to increase the number of images for a certain steering direction. Using this dataset, the vehicle should not tent to pull to one side, because the model has almost an equal number of images for turning left and right. 

First, all the necessary libraries are imported, for example csv to load the data from the csv file, cv2 for modifying the training images, numpy, random, matplotlib, sklearn, Tensorflow and keras. A batch size is set to 16. This is used later when the generator function is called. Then, the images are loaded from the csv file. An empty array for “lines” is created. The csv file “driving_log.csv” is then opened and every line from this file is appended to this array.
Afterwards, I used the train_test_split() function to separate an amount of 20 percent from the training data to have validation data. Then, the generator function is called, which uses the training samples and the batch size as input parameters. 
The generator function uses samples and the batch size as an input. It is used, to prevent memory errors when using too many images. Inside this function, the number of samples is first defined by calculating the number of samples using len(). Then, a correction factor is defined. I used 0.2 here. Using while 1, I made sure, that the generator never terminates. Afterwards, the samples are shuffled. Then, a for loop was used to check the data in steps the size of the batch size from zero to the number of samples defined earlier. For each of those steps, the batch data is extracted from the samples. Then, two empty arrays for images and the steering angle are created.
Using a for loop, every entry in the batch sample array is then looked at. For every line the path for the centre image is extracted by calling line[0], because the path for the centre image is in the first column. Using cv2.imread() the image is read and then appended to the “images” array. I also used the cv2.flip() function to flip the image along the vertical axis. Afterwards, the steering angle is read, converted to a float type and appended to the “angle” array. To get the right steering angle for the flipped image, the angle is multiplied with -1.0 and then also appended.
Since the simulator also recorded images from a left and right camera that can be useful for training, I also used them. For this, I first called path[1] for the right and path[2] for the left image. Then, the image was read by calling cv2.imread() again. The images were also appended to the “images” array. The corresponding steering angle was also appended, but with a correction factor of 0.2, which was added for the right image and subtracted for the left image. Similar to the images from the centre camera, I also used the flipped images from the left and right camera and appended those to the images array. Including the augmented images, the total number of training images is now doubled (48216 total), because every image is copied and flipped. After appending all steering angles, the two arrays are then converted to numpy arrays. The numpy array containing the images was called “X_train” and the array containing the steering angles. To check that the number of images is the same as the number of steering angles I printed the shapes of the arrays. Because this function is a generator, it has to end with the yield statement, instead of the return statement which is used for normal functions. The function ends with yield shuffle() and the X_train and Y_train array as an input.
Then, the model architecture was coded. I tried two variants for this. First, I wanted to implement the Nvidia architecture. I tested this architecture numerous times with different epoch numbers, layers and training images but never got a satisfying result. Therefore, I decided to not use the Nvidia architecture, but the LeNet5 architecture. For this, I added a Sequential model, followed by a lambda layer, which normalizes the images and a cropping2D() layer. Using the cropping 2D layer, I cut away the top 70 pixels and the lower 25 pixels of each image. This is useful, because the upper parts of the images contain scenery like trees or hills and the lower part contains part of the vehicle itself. These parts are not useful and should be deleted from the image since the model should use the lane markings to get the steering angle and not the scenery.
Then 3 convolutional were added – the first  two with a filter size of 5x5 and one with a filter size of 3x3. An ELU() layer was used after each convolutional layer as an activation function as well as a maxPooling2D() layer. Then, a flatten() layer and 4 dense() layers were added. To prevent overfitting, I inserted a dropout layer with a value of 0.2 after each dense layer. Afterwards, the model was compiled with an adam optimizer and a learning rate of 0.00015 and ‘mse’ as a loss function. To make use of the generator function described earlier, the fit_generator() function was used. I tried different number of epochs but choosing 3 seemed to be enough. Then, the structure of the model is plotted, and a summary of the model is printed. Finally, the model was saved using model.save() and choosing model.h5 as a name. 
2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing “python drive.py model.h5”. The result can be seen by opening the video.mp4 file.
3. Submission code is usable and readable
The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

